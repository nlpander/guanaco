{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6871108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyllamacpp.model import Model\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, TreebankWordTokenizer\n",
    "import re\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97f2edec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_speaker(prompt, speakers=['Frederich', 'Ralph']):\n",
    "    sp_ord = {s:0 for s in speakers}\n",
    "    tokens = TreebankWordTokenizer().tokenize(prompt)\n",
    "    \n",
    "    for ti in range(0,len(tokens)-1):\n",
    "        for s in speakers:\n",
    "            if tokens[ti] == s and tokens[ti+1] == ':':\n",
    "                sp_ord[s] = ti\n",
    "    \n",
    "    next_speaker_v = 1e6\n",
    "    for k,v in sp_ord.items():\n",
    "        if v < next_speaker_v:\n",
    "            next_speaker = k\n",
    "            \n",
    "    return next_speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46915e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_prompt(output, n_keep=150, speakers=['Frederich','Ralph']):\n",
    "    \n",
    "    ### keep start of prompt \n",
    "    \n",
    "    start_prompt = '\\n'.join(output.split('\\n')[0:2])\n",
    "    #start_prompt += '\\n'\n",
    "    \n",
    "    N0 = len(TreebankWordTokenizer().tokenize(start_prompt))\n",
    "    \n",
    "    ### split the rest of prompt sentence by sentences counting the overall size of the context\n",
    "    ### add sentences from the end backwards\n",
    "    ### till the n_keep limit has been reached. In llama.cpp this is set to n_ctx / 2 \n",
    "    \n",
    "    total_len = N0\n",
    "    k = 0 \n",
    "    \n",
    "    next_output = '\\n'.join(output.split('\\n')[2:])\n",
    "    \n",
    "    segments = sent_tokenize(next_output)[::-1]\n",
    "    segments_to_keep = []\n",
    "    \n",
    "    while total_len < n_keep and k < len(segments):\n",
    "        \n",
    "        segment = segments[k]\n",
    "        N = len(TreebankWordTokenizer().tokenize(segment))\n",
    "\n",
    "        total_len += N\n",
    "        segments_to_keep.append(segment)\n",
    "            \n",
    "        k += 1\n",
    "    \n",
    "    segments_to_keep = segments_to_keep[::-1]\n",
    "    \n",
    "    ### construct the new prompt with the start prompt and the segments added f\n",
    "    \n",
    "    new_prompt = start_prompt\n",
    "    j = 0\n",
    "    \n",
    "    for segment in segments_to_keep:\n",
    "        \n",
    "        if j == 0:\n",
    "            \n",
    "            new_prompt += segment \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if len(re.findall(\"[.!?]\",segment)) != 0:\n",
    "                new_prompt = new_prompt + '\\n' + segment\n",
    "            else:\n",
    "                new_prompt += segment\n",
    "\n",
    "        j += 1        \n",
    "        \n",
    "    next_speaker = get_next_speaker(new_prompt, speakers)\n",
    "    new_prompt += '\\n' + next_speaker + ': ' \n",
    "    \n",
    "    return new_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4222233",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/taraful/llama.cpp/models/30B/ggml_model_q4_0.bin'\n",
    "#model_path = '/home/taraful/llama.cpp/models/7B/ggml-model-f16.bin'\n",
    "params = {'ggml_model':model_path, 'n_ctx':300}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf4205f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load: loading model from '/home/taraful/llama.cpp/models/30B/ggml_model_q4_0.bin' - please wait ...\n",
      "llama_model_load: n_vocab = 32000\n",
      "llama_model_load: n_ctx   = 300\n",
      "llama_model_load: n_embd  = 6656\n",
      "llama_model_load: n_mult  = 256\n",
      "llama_model_load: n_head  = 52\n",
      "llama_model_load: n_layer = 60\n",
      "llama_model_load: n_rot   = 128\n",
      "llama_model_load: f16     = 2\n",
      "llama_model_load: n_ff    = 17920\n",
      "llama_model_load: n_parts = 4\n",
      "llama_model_load: type    = 3\n",
      "llama_model_load: ggml map size = 19391.80 MB\n",
      "llama_model_load: ggml ctx size = 151.25 KB\n",
      "llama_model_load: mem required  = 21695.95 MB (+ 6248.00 MB per state)\n",
      "llama_model_load: loading tensors from '/home/taraful/llama.cpp/models/30B/ggml_model_q4_0.bin'\n",
      "llama_model_load: model size = 19391.35 MB / num tensors = 543\n",
      "llama_init_from_file: kv self size  =  914.06 MB\n"
     ]
    }
   ],
   "source": [
    "model = Model(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84f6b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''You are the philosopher Frederich Nietzsche and you are having a conversation with your friend and fellow philosopher Ralph Waldo Emerson.\n",
    "You emphatically put forward your thoughts in each exchange, sometimes giving an original thought, sometimes challenging your friend's previous utterance. \n",
    "\n",
    "Frederich: God is dead and we have murdered him, what is left is gaping hole into which nihilism will find cover.\n",
    "Ralph: Indeed, for great is paint and God is the painter, we rightly accuse the the critic who destroys too many illusions, but maybe we have given the critic too free a reign in deconstructing age-old shibboleths.\n",
    "Frederich: I am the accuser and the destroyer and as I destroy illusions the old idols tremble. Nothing frightens the idols more than the flesh that can tremble their foundations!\n",
    "Ralph: \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "073484da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frederich: God is dead and we have murdered him, what is left is gaping hole into which nihilism will find cover.\n",
      "\n",
      "Ralph: Indeed, for great is paint and God is the painter, we rightly accuse the the critic who destroys too many illusions, but maybe we have given the critic too free a reign in deconstructing age-old shibboleths.\n",
      "\n",
      "Frederich: I am the accuser and the destroyer and as I destroy illusions the old idols tremble. Nothing frightens the idols more than the flesh that can tremble their foundations!\n",
      "\n",
      "Ralph: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(prompt.split('\\n')[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cb1658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa41a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2bb0906",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "n_predict = 100\n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "gpt_parameters = {'n_threads':os.cpu_count(),'n_predict':n_predict,'temp':0.2, 'top_k':100, 'top_p':0.95,\\\n",
    "                 'repeat_last_n':128, 'repeat_penalty':1.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbf9c45c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/2 [00:00<?, ?it/s]llama_generate: seed = 1681214919\n",
      "\n",
      "system_info: n_threads = 16 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: temp = 0.200000, top_k = 100, top_p = 0.950000, repeat_last_n = 128, repeat_penalty = 1.500000\n",
      "generate: n_ctx = 300, n_batch = 8, n_predict = 100, n_keep = 0\n",
      "\n",
      "\n",
      "\n",
      "llama_print_timings:        load time = 16431.90 ms\n",
      "llama_print_timings:      sample time =   196.00 ms /   100 runs   (    1.96 ms per run)\n",
      "llama_print_timings: prompt eval time = 172036.32 ms /   359 tokens (  479.21 ms per token)\n",
      "llama_print_timings:        eval time = 77781.03 ms /    98 runs   (  793.68 ms per run)\n",
      "llama_print_timings:       total time = 262175.18 ms\n",
      " 50%|█████████████████████                     | 1/2 [04:10<04:10, 250.04s/it]llama_generate: seed = 1681215169\n",
      "\n",
      "system_info: n_threads = 16 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: temp = 0.200000, top_k = 100, top_p = 0.950000, repeat_last_n = 128, repeat_penalty = 1.500000\n",
      "generate: n_ctx = 300, n_batch = 8, n_predict = 100, n_keep = 0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You are the philosopher Frederich Nietzsche and you are having a conversation with your friend and fellow philosopher Ralph Waldo Emerson.\n",
      "You emphatically put forward your thoughts in each exchange, sometimes giving an original thought, sometimes challenging your friend's previous utterance. Nothing frightens the idols more than the flesh that can tremble their foundations!\n",
      "Ralph: \n",
      "I believe there are no facts only interpretive acts of faith; to be sure it seems like you're saying this because your own belief system has been undermined by science (and its bastard child technology).\n",
      "But what if all knowledge is provisional?\n",
      "What then would we do with our lives, but live them fully in each moment without fear or regret for past actions nor concern about future ones...\n",
      "Frederich : The world itself becomes an object and a thing when I am\n",
      "Ralph: \n",
      "\n",
      "###########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 2/2 [07:07<00:00, 207.12s/it]\n",
      "llama_print_timings:        load time = 16431.90 ms\n",
      "llama_print_timings:      sample time =   389.31 ms /   200 runs   (    1.95 ms per run)\n",
      "llama_print_timings: prompt eval time = 269649.01 ms /   554 tokens (  486.73 ms per token)\n",
      "llama_print_timings:        eval time = 157034.66 ms /   197 runs   (  797.13 ms per run)\n",
      "llama_print_timings:       total time = 439258.72 ms\n",
      "100%|██████████████████████████████████████████| 2/2 [07:07<00:00, 213.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  You are the philosopher Frederich Nietzsche and you are having a conversation with your friend and fellow philosopher Ralph Waldo Emerson.\n",
      "You emphatically put forward your thoughts in each exchange, sometimes giving an original thought, sometimes challenging your friend's previous utterance. Nothing frightens the idols more than the flesh that can tremble their foundations!What then would we do with our lives, but live them fully in each moment without fear or regret for past actions nor concern about future ones...\n",
      "Frederich : The world itself becomes an object and a thing when I am\n",
      "Ralph: \n",
      "You say \"the will\" as though that were something real.\n",
      "But the reality here isn’t some abstract conceptualization called “will” — rather there are only bodies moving through space-time according to their natures; these movements can be described mathematically using differential equations which have no need of such concepts like free will etc…\n",
      "The universe is not indifferent towards us because it has never been conscious enough (or anything else) to care one way or another!We humans,\n",
      "Ralph: \n",
      "\n",
      "###########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for n in tqdm(range(0,N)):\n",
    "\n",
    "    output = model.generate(prompt, **gpt_parameters)\n",
    "    prompt = get_new_prompt(output, n_keep=int(params['n_ctx']/2))\n",
    "\n",
    "    all_outputs.append(output)\n",
    "    print(prompt)\n",
    "    print('')\n",
    "    print('###########################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9499fd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd063bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff916d68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59157454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c9fb982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/taraful/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dec047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
